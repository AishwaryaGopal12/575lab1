{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 575 - Advanced Machine Learning\n",
    "\n",
    "# Lab 1: Text generation using Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "- [Submission guidelines](#sg) (4%)\n",
    "- [Exercise 1: Warm-up](#1) (20%)\n",
    "- [Exercise 2: Character-based Markov model of language](#2) (20%)\n",
    "- [Exercise 3: Stationary distribution and other fun stuff](#3) (30%)\n",
    "- [Exercise 4: Word-based Markov model of language](#4) (26%)\n",
    "- [Submission to Canvas](#sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions <a name=\"si\"></a>\n",
    "<hr>\n",
    "rubric={mechanics:4}\n",
    "\n",
    "You will receive marks for correctly submitting this assignment. \n",
    "\n",
    "To correctly submit this assignment follow the instructions below:\n",
    "\n",
    "- Push your assignment to your GitHub repository. \n",
    "- Add a link to your GitHub repository here: LINK TO YOUR GITHUB REPO \n",
    "- Upload an HTML render of your assignment to Canvas. The last cell of this notebook will help you do that.\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/).\n",
    "\n",
    "[Here](https://github.com/UBC-MDS/public/tree/master/rubric) you will find the description of each rubric used in MDS.\n",
    "\n",
    "**NOTE: The data you download for use in this lab SHOULD NOT BE PUSHED TO YOUR REPOSITORY. You might be penalised for pushing datasets to your repository. I have seeded the repository with `.gitignore` and hoping that it won't let you push CSVs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports <a name=\"im\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Warm-up <a name=\"1\"></a>\n",
    "\n",
    "This exercise will get you thinking about how to generate text using Markov models. In the lecture, we built a Markov model of language on a toy corpus using words. In this exercise, you will build a letter or character-based Markov model of language with a tiny corpus. The goal of such a model is to predict next character given a sequence of characters. \n",
    "\n",
    "In NLP, a Markov model of language is also referred to as **an n-gram language model**. In this exercise we'll focus on a character-based bigram (2-gram) language model. We will use the variable `n=1` to denote that we are only considering the current character in the sequence to predict the next character. In the next exercise, you'll explore different values for `n` in your n-gram language model. \n",
    " \n",
    "As we saw in class, to build a bigram model, we need bigram frequency counts, i.e., counts of all 2-letter sequences in your corpus. Below I am providing you some starter code to create bigram frequency counts of letters in a tiny corpus with 6 words. Our goal is to build a character-based bigram language model with this corpus, where the set of states is the unique letters in the corpus.  \n",
    "\n",
    "> When we say n-gram we're now referring to the last `n` characters, not the last `n` words. In general, the term n-gram can refer to characters or word; see the [Wikipedia article](https://en.wikipedia.org/wiki/N-gram).\n",
    "\n",
    "\n",
    "> Recall that in [DSCI 512 lab1](https://github.ubc.ca/MDS-2020-21/DSCI_512_alg-data-struct_students/blob/master/solutions/lab1/lab1.ipynb), you implemented a Markov model of language and generated text (without actually knowing the details about the model). You pre-computed the frequencies for every possible `n` gram as follows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"to be or not to be\"  # our tiny corpus\n",
    "n = 1  # for bigrams\n",
    "circ_corpus = corpus + corpus[:n]\n",
    "frequencies = defaultdict(Counter)\n",
    "for i in range(len(circ_corpus) - n):\n",
    "    frequencies[circ_corpus[i : i + n]][circ_corpus[i + n]] += 1\n",
    "frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Visualizing character bigram counts as a co-occurrence matrix\n",
    "rubric={accuracy:4} \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Show the bigram frequencies in `frequencies` variable above as a pandas DataFrame, sorting the column and row labels alphabetically, where\n",
    "    * column labels and row indices are unique characters in the corpus, and \n",
    "    * the value in each cell $a_{ij}$ represents how often the character $i$ precedes character $j$ in our corpus.\n",
    "    \n",
    "> Note: Fill in the NaN values with zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution_1_1_1\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Transition matrix\n",
    "rubric={accuracy:6}\n",
    "\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Given the frequencies in 1.1, compute the transition matrix (i.e., the conditional probability distribution for every possible bigram). Visualize the transition matrix as a pandas DataFrame.  \n",
    "2. How many unique states are there in your Markov model? \n",
    "\n",
    "> Recall that the transition matrix $T$ is a square matrix and the number of rows/columns is equal to the number of states. Each row is a discrete probability distribution summing to 1. The element $T_{ij}$ is the probability of transitioning from state $i$ to state $j$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### Solution_1_2_1\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution_1_2_2\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Probability of sequences\n",
    "rubric={reasoning:4}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Suppose the probability of starting a sequence with letter $b$ is $0.4$ (i.e., $\\pi_0$ of state $b$ = 0.4). Given the transition matrix in 1.2, calculate the probabilities for the following sequences. \n",
    "\n",
    "1. \"be or\"\n",
    "2. \"beet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution_1_3**\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Generate sequences\n",
    "rubric={reasoning:6}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Given the transition matrix above, explain in words how would you generate a sequence of length `seq_len` given the seed letter `t`. You don't have to, but you are welcome to write code for this. \n",
    "2. Give a few example applications for word- or character-level text generation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution_1_4_1**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution_1_4_1**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Character-based Markov model of language <a name=\"2\"></a>\n",
    "\n",
    "Recall Markov Model of language from [DSCI 512 lab1](https://github.ubc.ca/MDS-2020-21/DSCI_512_alg-data-struct_students/blob/master/solutions/lab1/lab1.ipynb). In that lab, you wrote a class `MarkovModel` to `fit` an n-gram Markov model and generated text. Now that you know what Markov models are, we will start with what you did in DSCI 512 lab1 and build on it. \n",
    "\n",
    "The starter code below uses the hyperparameter `n`, where our \"state\" of the Markov chain is the last `n` characters of a give string. In Exercise 1, we worked with `n=1` (bigram model) and our \"state\" of the Markov chain was a single character and each character was dependent on the last one character. When `n=3`, it means that the probability distribution over the _next character_ only depends on the _preceding 3 characters_. \n",
    "\n",
    "> Note that `n` in n-gram doesn't exactly correspond to the variable `n` we are using in the implementation below. For 2-gram (bigram) the value of the variable `n` is 1, and for 4-gram the value of `n=3`. \n",
    "\n",
    "We train our model from data by recording every occurrence of every n-gram and, in each case, recording what the next letter is. Then, for each n-gram, we normalize these counts into probabilities just like we did with naive Bayes. The `fit` function below implements these steps.\n",
    "\n",
    "To generate a new sequence, we start with some initial seed at least of length `n` (here, we will just use the first `n` characters in the training text, which are saved at the end of the `fit` function). Then, for the current n-gram we will look up the probability distribution over next characters and sample a random character according to this distribution.\n",
    "\n",
    "Attribution: assignment adapted with permission from Princeton COS 126, [_Markov Model of Natural Language_]( http://www.cs.princeton.edu/courses/archive/fall15/cos126/assignments/markov.html). Original assignment was developed by Bob Sedgewick and Kevin Wayne. If you are interested in more background info, you can take a look at the original version. The original paper by Shannon, [A Mathematical Theory of Communication](http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf), essentially created the field of information theory and is one of the best scientific papers ever written (in terms of both impact and readability).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovModel:\n",
    "    def __init__(self, n):\n",
    "        \"\"\"\n",
    "        Initialize the Markov model object.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n : int\n",
    "            the size of the ngram\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "\n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        Fit a Markov chain and create a transition matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            text to fit the Markov chain\n",
    "        \"\"\"\n",
    "\n",
    "        # make text circular so markov chain doesn't get stuck\n",
    "        circ_text = text + text[: self.n]\n",
    "\n",
    "        # count the number of occurrences of each letter following a given n-gram\n",
    "        frequencies = defaultdict(Counter)\n",
    "        for i in range(len(text)):\n",
    "            frequencies[circ_text[i : i + self.n]][circ_text[i + self.n]] += 1.0\n",
    "\n",
    "        # normalize the frequencies into probabilities (separately for each n-gram)\n",
    "        self.probabilities_ = defaultdict(dict)\n",
    "        for ngram, counts in frequencies.items():\n",
    "            self.probabilities_[ngram][\"symbols\"] = list(counts.keys())\n",
    "            probs = np.array(list(counts.values()))\n",
    "            probs /= np.sum(probs)\n",
    "            self.probabilities_[ngram][\"probs\"] = probs\n",
    "\n",
    "        # store the first n characters of the training text, as we will use these\n",
    "        # to seed our `generate` function\n",
    "        self.starting_chars = text[: self.n]\n",
    "        self.frequencies_ = frequencies  # you never know when this might come in handy\n",
    "\n",
    "    def generate(self, seq_len):\n",
    "        \"\"\"\n",
    "        Using self.starting_chars, generate a sequence of length seq_len\n",
    "        using the transition matrix created in the fit method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seq_len : int\n",
    "            the desired length of the sequence\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        str\n",
    "            the generated sequence\n",
    "        \"\"\"\n",
    "        s = self.starting_chars\n",
    "        while len(s) < seq_len:\n",
    "            probs = self.probabilities_[s[-self.n :]]\n",
    "            s += npr.choice(probs[\"symbols\"], p=probs[\"probs\"])\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grimms' Fairy Tales by Jacob Grimm and Wilhelm Grimm\n",
    "data_url = \"http://www.gutenberg.org/files/2591/2591-0.txt\"\n",
    "\n",
    "corpus = urlopen(data_url).read().decode(\"utf-8\")\n",
    "corpus = corpus[2000:]\n",
    "model = MarkovModel(n=3)\n",
    "model.fit(corpus)\n",
    "print(model.generate(500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 \"Circular\" version of the text\n",
    "rubric={reasoning:5}\n",
    "\n",
    "**Your taks:**\n",
    "\n",
    "Why do we need to use a \"circular\" version of the text in the `fit` function? What could go wrong if we didn't do that, and instead used the original `text` (instead of `circ_text`) and made the loop:  \n",
    "```\n",
    "for i in range(len(text)-self.n):\n",
    "```\n",
    "which allow `fit` to run without crashing?\n",
    "\n",
    "> Hint: the problem would arise during `generate`, not during `fit`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution_2.1**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) 2.2 Smoothing\n",
    "rubric={reasoning:1}\n",
    "\n",
    "The description above mentioned a connection to naive Bayes. When discussing naive Bayes we briefly mentioned [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing). Is there something analogous to Laplace smoothing that we could do here? If so, describe what it would entail and what effect it would have. (You don't have to implement it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution_2.2**\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 States, state space, and transition matrix\n",
    "rubric={reasoning:15}\n",
    "\n",
    "Let's consider the Markov chain interpretation of what we've just done. Let's define a state as the previous $n$ characters \"emitted\" by our chain. Let's assume our vocabulary size (the number of possible characters) is $V$; for example, $V=26$ if we were only using lowercase letters. We can compute this in our case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocabulary size = %d\" % len(np.unique(list(corpus))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's just stick with a general $V$ for now. Call the number of possible states $S$. Let's consider the _transition matrix_ $T$ of this Markov chain. The transition matrix is a square matrix and the number of rows/columns is equal to the number of states, so in our case it is $S \\times S$. Each row is a discrete probability distribution summing to 1. The element $T_{ij}$ is the probability of transitioning from state $i$ to state $j$. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. What is the number of possible states, $S$, in terms of $V$ and $n$? (You might not encounter all these states in practice, if not all $n$-grams appear in the training data, but let's ignore that for now.)\n",
    "2. Give two examples states in the state space for $n=3$.\n",
    "3. In our application above when $n>1$ _not all transitions are possible_. This means $T$ will contain a bunch of zeros. Why?\n",
    "4. Again for $n>1$ what is the maximum number of nonzero elements that $T$ could have? Answer in terms of $V$ and $n$. \n",
    "5. Is the state space discrete or continuous?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Stationary distribution and other fun stuff <a name=\"3\"></a>\n",
    "\n",
    "The code below computes the transition matrix for you for the Shakespeare data with `n=1`. Consider this transition matrix for the next few questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Project Gutenberg eBook of The Complete Works of William Shakespeare, by William Shakespeare\n",
    "data_url = \"http://www.gutenberg.org/files/100/100-0.txt\"\n",
    "shakespeare_text = urlopen(data_url).read().decode(\"utf-8\")\n",
    "shakespeare_text = shakespeare_text[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.unique(list(shakespeare_text))\n",
    "print(\"States:\", states)\n",
    "S = len(np.unique(list(shakespeare_text)))\n",
    "print(\"Number of states:\", S)\n",
    "\n",
    "model = MarkovModel(n=1)\n",
    "model.fit(shakespeare_text)\n",
    "\n",
    "# implementation note: since len(model.probabilities_[state][\"probs\"]) might not equal S\n",
    "# for all letters, we need to be careful and do a reverse-lookup for the actual letters\n",
    "# the rest of the transition probabilities are just zero (if they don't appear)\n",
    "lookup = dict(zip(states, list(np.arange(S, dtype=int))))\n",
    "\n",
    "T = np.zeros((S, S))  # transition matrix\n",
    "for i, state in enumerate(states):\n",
    "    for j, letter in enumerate(model.probabilities_[state][\"symbols\"]):\n",
    "        T[i, lookup[letter]] = model.probabilities_[state][\"probs\"][j]\n",
    "\n",
    "print(\"Number of nonzero transition probabilities: %d/%d\" % (np.sum(T > 0), T.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Stationary distribution conditions \n",
    "rubric={reasoning:5}\n",
    "\n",
    "Under mild assumptions, a Markov chain has a _stationary distribution_ which is the probability of finding yourself in the same state after the chain is run for a long time. These assumptions are:\n",
    "\n",
    "- \"Irreducibility\" (doesn’t get stuck in part of the graph)\n",
    "- \"Aperiodicity\" (doesn’t keep repeating same sequence).\n",
    "\n",
    "Give a short explanation for why our Markov chain might satisfy these assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Stationary distribution for the `shakespeare_text`\n",
    "rubric={accuracy:10}\n",
    "\n",
    "It's not true in general but in this particular case we actually already know the stationary distribution -- it is just the relative frequency that these n-grams occur in the training data (you are welcome to think about this deeply, but you don't have to). (Recall that we are assuming n=1 here.)\n",
    "\n",
    "**Your tasks:** \n",
    "\n",
    "1. Calculate the stationary distribution for the `shakespeare_text` by calculating the relative frequencies for each state in the corpus (unique letter in this case, as we are assuming `n=1`).\n",
    "2. Show empirically that this is a stationary distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 3.3 Stationary distribution using eigenvalue decomposition\n",
    "rubric={reasoning:1}\n",
    "\n",
    "You can compute the stationary distribution in another ways. \n",
    "\n",
    "**Your tasks**\n",
    "\n",
    "1. Calculate stationary distribution using eigenvalue decomposition of the transition matrix. Compare your result from part 3.2 with the result below. Do they agree with each other?\n",
    "\n",
    "> We haven't explicitly talked about this in class but at this point in the program, I believe that you are independent enough to do it on your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution_3_3_1\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Finding probability of occurrences of patterns \n",
    "rubric={reasoning:15}\n",
    "\n",
    "Let's consider the conditional probability that a lower case vowel comes 3 characters later given that the current letter is \"a\". In other words, we're searching for the pattern `axxv` where `v` is a lower case vowel (defined as a,e,i,o,u,y) and `x` is any character and `a` is literally \"a\". \n",
    "\n",
    "Let's use `n=1`.\n",
    "\n",
    "**Your tasks:**\n",
    "1. It turns out we can estimate this probability directly from the transition matrix. While $T$ gives us the probabilities one step ahead, $T\\times T$ gives us the probabilities 2 steps ahead, etc. (If you want to think about this, you should be able to convince yourself it's true by contemplating what matrix multiplication really means, but this is optional.) So, taking $T^3$ gives us the transition probabilities for 3 steps later. Compute $T^3$ and find the estimated probability that we're looking for. \n",
    "2. We could also estimate this probability directly from the data, by just seeing how frequently this pattern occurs. Do this. How well does it match your above results? What does this tell us about how reasonable our  Markov assumption is?\n",
    "3. What if we increased `n` and repeated part (1), would you expect to get closer to the answer from part (2)? You are welcome to try this but you don't have to - the goal is just to think about it.\n",
    "\n",
    "> Note for 3.4.1: you should NOT use `T**3`, as that is element-wise exponentiation rather than a matrix power. You can get $T^3$ using `numpy.linalg.matrix_power(T,3)` or just `T@T@T`.\n",
    "\n",
    "> This question might be a bit tricky. But hopefully it will help you understand Markov models better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution_3_4_1\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution_3_4_2\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution_3_4_3**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Markov model of language with words <a name=\"4\"></a>\n",
    "\n",
    "In this exercise we'll continue with the Markov model of language, but we'll work with _words_ instead of _characters_. The `MarkovModel` code stays the same. Just remember that now we are dealing with words and not characters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we say n-gram we're now referring to `n` words, not `n` characters.\n",
    "\n",
    "One concern with words is that, in a sense, we have less examples to work with. For example, with character n-grams and `n=3` we have a lot of examples of seeing the character combination \"per\" and checking what character follows it, but with words even with `n=1` (the smallest nontrivial value of `n`), we might only have one example of the word \"persuade\" in our corpus. You can imagine that the number of \"training examples\" only gets smaller if `n`>1. This is something to keep in mind when deciding what values of `n` might be reasonable.\n",
    "\n",
    "Another issue is that the number of states could explode when you are working with words. For example, with character bigram model our states were all unique characters in the text (107 in our example above). For word bigram model, it's going to be the number of unique words in the corpus, which would be a much bigger number. You can imagine that for a large corpus and bigger values of `n`, the number of states could explode.  \n",
    "\n",
    "Finally, we need to preprocess the text (i.e., tokenization) before creating a word-based n-gram model. To accomplish this preprocessing, we will use the popular python package [NLTK](http://www.nltk.org/) (Natural Language ToolKit), which we have seen in class. If you are using the course conda environment, you should already have the package in your conda environment. \n",
    "\n",
    "You might need to install the NLTK data files with the following command in your conda environment in the terminal.\n",
    "\n",
    "```\n",
    "python -m nltk.downloader 'punkt'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Word-based n-gram language model\n",
    "rubric={accuracy:8,reasoning:4}\n",
    "\n",
    "The first step is to break apart the text string into words. NLTK's `word_tokenize` will turn our text into a lists of \"tokens\" which we will use as our language units. This tokenizing process removes whitespace. So, in `generate`, you should add a space character back in between tokens. This won't look quite right because you'll have spaces before punctuations, but it's good enough for now.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Implement a word-based Markov model of language.\n",
    "2. Train your word-based Markov model on `shakespeare_text` above with your choice of `n` and generate some text. \n",
    "3. Discuss the differences between character-based and word-based Markov models in terms of generated text, the number of states, and the time taken to generate the text. \n",
    "\n",
    "> Note: You can reuse the code from Exercise 2. Only the `generate` function needs to be rewritten, and, even there, the required modifications are not large. You may use class inheritance you have learned in 511 here. The line\n",
    "```\n",
    "class MarkovModelWords(MarkovModel):\n",
    "```\n",
    "> defines a new class called `MarkovModelWords` that you can think of as an offspring of `MarkovModel`. It \"inherits\" all the functions from `MarkovModel` unless we explicitly overwrite them. So, if you implement `generate` in the class `MarkovModelWords`, the old `generate` function will be overwritten but all the other functions will persist. But, if you're not comfortable with inheritance, or just don't feel like it, you're  welcome to just modify the above code and not use inheritance.\n",
    "\n",
    "\n",
    "> Another note: The way `MarkovModel` is implemented, the patterns we're conditioning on (the last $n$ characters/words) are used as keys in python dictionaries. For characters that was easy because we could just use a string as a key. However, for words with $n>1$ we now have a collection of words, and we want to use this as a dict key. `nltk.tokenize.word_tokenize` outputs a list, which can't be used as a key. I've casted the list to a tuple in the code below, because a tuple _can_ be used as a dict key (if you case: I believe this is because it's immutable whereas a list is not). This tiny change allows reuse of the old code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text_tok = word_tokenize(shakespeare_text)\n",
    "text_tok = tuple(text_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution_4_1_1\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution_4_1_2\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution_4_1_3**\n",
    "\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Dealing with punctuation\n",
    "rubric={accuracy:10,reasoning:4}\n",
    "\n",
    "If you've generated text from 4.1, you probably noticed that the way whitespace is handled is not ideal. In particular, whitespace is added after every token, including right before punctuation. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Modify your code to fix this and show a generated sequence. You can access a list of punctuation characters with `string.punctuation` from the [`string` library](https://docs.python.org/3/library/string.html).\n",
    "2. Generate text using different values of `n` on `shakespeare_text`.  \n",
    "3. Train word-based Markov models on another corpus of your choice and generate text with different values of `n`. You may use any corpus of your choice. For inspiration, you may find some corpora [here](http://www.gutenberg.org/). \n",
    "4. Note your observations on \n",
    "    - how the generated text changes based on the corpus \n",
    "    - how the value of `n` affects the quality of the generated text and time taken to generate the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution_4_2_1\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution_4_2_2\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution_4_2_3\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution_4_2_4**\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 4.3 Other improvements\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Are there other improvements you'd like to make to your word-based Markov model so that the generated text looks like proper English? Feel free to either implement them or just describe what you might want to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution_4_3**\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission to Canvas <a name=\"sc\"></a>\n",
    "\n",
    "**PLEASE READ: When you are ready to submit your assignment do the following:**\n",
    "\n",
    "- Run all cells in your notebook to make sure there are no errors by doing Kernel -->  Restart Kernel and Run All Cells...\n",
    "- If you are using the \"575\" `conda` environment, make sure to select it before running all cells. \n",
    "- Convert your notebook to .html format using the `convert_notebook()` function below or by File -> Export Notebook As... -> Export Notebook to HTML\n",
    "- Run the code `submit()` below to go through an interactive submission process to Canvas.\n",
    "After submission, be sure to do a final push of all your work to GitHub (including the rendered html file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from canvasutils.submit import convert_notebook, submit\n",
    "\n",
    "# convert_notebook(\"lab1.ipynb\", \"html\")  # uncomment and run when you want to try convert your notebook (or you can convert manually from the File menu)\n",
    "# submit(course_code=65512, token=False)  # uncomment and run when ready to submit to Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done!! Congratulations on finishing the lab!! "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:575]",
   "language": "python",
   "name": "conda-env-575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
