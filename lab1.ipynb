{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 575 - Advanced Machine Learning\n",
    "\n",
    "# Lab 1: Text generation using Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "- [Submission guidelines](#sg) (4%)\n",
    "- [Exercise 1: Warm-up](#1) (20%)\n",
    "- [Exercise 2: Character-based Markov model of language](#2) (20%)\n",
    "- [Exercise 3: Stationary distribution and other fun stuff](#3) (30%)\n",
    "- [Exercise 4: Word-based Markov model of language](#4) (26%)\n",
    "- [Submission to Canvas](#sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions <a name=\"si\"></a>\n",
    "<hr>\n",
    "rubric={mechanics:4}\n",
    "\n",
    "You will receive marks for correctly submitting this assignment. \n",
    "\n",
    "To correctly submit this assignment follow the instructions below:\n",
    "\n",
    "- Push your assignment to your GitHub repository. \n",
    "- Add a link to your GitHub repository here: LINK TO YOUR GITHUB REPO  https://github.ubc.ca/MDS-2020-21/DSCI_575_lab1_aish912\n",
    "- Upload an HTML render of your assignment to Canvas. The last cell of this notebook will help you do that.\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/).\n",
    "\n",
    "[Here](https://github.com/UBC-MDS/public/tree/master/rubric) you will find the description of each rubric used in MDS.\n",
    "\n",
    "**NOTE: The data you download for use in this lab SHOULD NOT BE PUSHED TO YOUR REPOSITORY. You might be penalised for pushing datasets to your repository. I have seeded the repository with `.gitignore` and hoping that it won't let you push CSVs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports <a name=\"im\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Warm-up <a name=\"1\"></a>\n",
    "\n",
    "This exercise will get you thinking about how to generate text using Markov models. In the lecture, we built a Markov model of language on a toy corpus using words. In this exercise, you will build a letter or character-based Markov model of language with a tiny corpus. The goal of such a model is to predict next character given a sequence of characters. \n",
    "\n",
    "In NLP, a Markov model of language is also referred to as **an n-gram language model**. In this exercise we'll focus on a character-based bigram (2-gram) language model. We will use the variable `n=1` to denote that we are only considering the current character in the sequence to predict the next character. In the next exercise, you'll explore different values for `n` in your n-gram language model. \n",
    " \n",
    "As we saw in class, to build a bigram model, we need bigram frequency counts, i.e., counts of all 2-letter sequences in your corpus. Below I am providing you some starter code to create bigram frequency counts of letters in a tiny corpus with 6 words. Our goal is to build a character-based bigram language model with this corpus, where the set of states is the unique letters in the corpus.  \n",
    "\n",
    "> When we say n-gram we're now referring to the last `n` characters, not the last `n` words. In general, the term n-gram can refer to characters or word; see the [Wikipedia article](https://en.wikipedia.org/wiki/N-gram).\n",
    "\n",
    "\n",
    "> Recall that in [DSCI 512 lab1](https://github.ubc.ca/MDS-2020-21/DSCI_512_alg-data-struct_students/blob/master/solutions/lab1/lab1.ipynb), you implemented a Markov model of language and generated text (without actually knowing the details about the model). You pre-computed the frequencies for every possible `n` gram as follows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'t': Counter({'o': 2, ' ': 1}),\n",
       "             'o': Counter({' ': 2, 'r': 1, 't': 1}),\n",
       "             ' ': Counter({'b': 2, 'o': 1, 'n': 1, 't': 1}),\n",
       "             'b': Counter({'e': 2}),\n",
       "             'e': Counter({' ': 1, 't': 1}),\n",
       "             'r': Counter({' ': 1}),\n",
       "             'n': Counter({'o': 1})})"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = \"to be or not to be\"  # our tiny corpus\n",
    "n = 1  # for bigrams\n",
    "circ_corpus = corpus + corpus[:n]\n",
    "frequencies = defaultdict(Counter)\n",
    "for i in range(len(circ_corpus) - n):\n",
    "    frequencies[circ_corpus[i : i + n]][circ_corpus[i + n]] += 1\n",
    "frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Visualizing character bigram counts as a co-occurrence matrix\n",
    "rubric={accuracy:4} \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Show the bigram frequencies in `frequencies` variable above as a pandas DataFrame, sorting the column and row labels alphabetically, where\n",
    "    * column labels and row indices are unique characters in the corpus, and \n",
    "    * the value in each cell $a_{ij}$ represents how often the character $i$ precedes character $j$ in our corpus.\n",
    "    \n",
    "> Note: Fill in the NaN values with zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>e</th>\n",
       "      <th>n</th>\n",
       "      <th>o</th>\n",
       "      <th>r</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          b    e    n    o    r    t\n",
       "   0.0  2.0  0.0  1.0  1.0  0.0  1.0\n",
       "b  0.0  0.0  2.0  0.0  0.0  0.0  0.0\n",
       "e  1.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
       "n  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
       "o  2.0  0.0  0.0  0.0  0.0  1.0  1.0\n",
       "r  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "t  1.0  0.0  0.0  0.0  2.0  0.0  0.0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Solution_1_1_1\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "frequencies_df = pd.DataFrame.from_dict(frequencies, orient = 'index')\n",
    "frequencies_df.sort_index(axis=1, inplace=True)\n",
    "frequencies_df.sort_index(axis=0, inplace=True)\n",
    "frequencies_df = frequencies_df.replace(np.nan, 0)\n",
    "frequencies_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Transition matrix\n",
    "rubric={accuracy:6}\n",
    "\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Given the frequencies in 1.1, compute the transition matrix (i.e., the conditional probability distribution for every possible bigram). Visualize the transition matrix as a pandas DataFrame.  \n",
    "2. How many unique states are there in your Markov model? \n",
    "\n",
    "> Recall that the transition matrix $T$ is a square matrix and the number of rows/columns is equal to the number of states. Each row is a discrete probability distribution summing to 1. The element $T_{ij}$ is the probability of transitioning from state $i$ to state $j$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>e</th>\n",
       "      <th>n</th>\n",
       "      <th>o</th>\n",
       "      <th>r</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           b    e    n     o     r     t\n",
       "   0.00  0.4  0.0  0.2  0.20  0.00  0.20\n",
       "b  0.00  0.0  1.0  0.0  0.00  0.00  0.00\n",
       "e  0.50  0.0  0.0  0.0  0.00  0.00  0.50\n",
       "n  0.00  0.0  0.0  0.0  1.00  0.00  0.00\n",
       "o  0.50  0.0  0.0  0.0  0.00  0.25  0.25\n",
       "r  1.00  0.0  0.0  0.0  0.00  0.00  0.00\n",
       "t  0.33  0.0  0.0  0.0  0.67  0.00  0.00"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Solution_1_2_1\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "for index, row in frequencies_df.iterrows():\n",
    "    n = np.nansum(row)\n",
    "    row[:] = [round(f/n,2) for f in row]\n",
    "frequencies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution_1_2_2\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "There are seven unique states in the markov model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Probability of sequences\n",
    "rubric={reasoning:4}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Suppose the probability of starting a sequence with letter $b$ is $0.4$ (i.e., $\\pi_0$ of state $b$ = 0.4). Given the transition matrix in 1.2, calculate the probabilities for the following sequences. \n",
    "\n",
    "1. \"be or\"\n",
    "2. \"beet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution_1_3**\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "1. P(b) * P(e|b) * P( |e) * P(o| ) * P(r|o) \n",
    "\n",
    "   0.4 * 1.0 * 0.5 * 0.2 * 0.25 = \n",
    "   \n",
    "  \n",
    "2. P(b) * P(e|b) * P(e|e) * P(t|e) \n",
    "\n",
    "   0.4 * 1.0 * 0 * 0.5\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Generate sequences\n",
    "rubric={reasoning:6}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Given the transition matrix above, explain in words how would you generate a sequence of length `seq_len` given the seed letter `t`. You don't have to, but you are welcome to write code for this. \n",
    "2. Give a few example applications for word- or character-level text generation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution_1_4_1**\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "Given the first character is t, from the transition matrix, we can observe that, the following charcters can either be a space or the letter 'n'. Once the next letter is chosen from the distribution of \" \" and \"n\", that character becomes the current state. For instance, if n is chosen, then from the transition matrix, the possible character is only \"o\". This becomes the current state. This process continues until the length condition is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution_1_4_1**\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "An application of character-level text generation is the auto-complete in our text applications.\n",
    "\n",
    "Application of word-level text generation is the gmail autocomplete feature or the dropdown autocomplete in youtube, google etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Character-based Markov model of language <a name=\"2\"></a>\n",
    "\n",
    "Recall Markov Model of language from [DSCI 512 lab1](https://github.ubc.ca/MDS-2020-21/DSCI_512_alg-data-struct_students/blob/master/solutions/lab1/lab1.ipynb). In that lab, you wrote a class `MarkovModel` to `fit` an n-gram Markov model and generated text. Now that you know what Markov models are, we will start with what you did in DSCI 512 lab1 and build on it. \n",
    "\n",
    "The starter code below uses the hyperparameter `n`, where our \"state\" of the Markov chain is the last `n` characters of a give string. In Exercise 1, we worked with `n=1` (bigram model) and our \"state\" of the Markov chain was a single character and each character was dependent on the last one character. When `n=3`, it means that the probability distribution over the _next character_ only depends on the _preceding 3 characters_. \n",
    "\n",
    "> Note that `n` in n-gram doesn't exactly correspond to the variable `n` we are using in the implementation below. For 2-gram (bigram) the value of the variable `n` is 1, and for 4-gram the value of `n=3`. \n",
    "\n",
    "We train our model from data by recording every occurrence of every n-gram and, in each case, recording what the next letter is. Then, for each n-gram, we normalize these counts into probabilities just like we did with naive Bayes. The `fit` function below implements these steps.\n",
    "\n",
    "To generate a new sequence, we start with some initial seed at least of length `n` (here, we will just use the first `n` characters in the training text, which are saved at the end of the `fit` function). Then, for the current n-gram we will look up the probability distribution over next characters and sample a random character according to this distribution.\n",
    "\n",
    "Attribution: assignment adapted with permission from Princeton COS 126, [_Markov Model of Natural Language_]( http://www.cs.princeton.edu/courses/archive/fall15/cos126/assignments/markov.html). Original assignment was developed by Bob Sedgewick and Kevin Wayne. If you are interested in more background info, you can take a look at the original version. The original paper by Shannon, [A Mathematical Theory of Communication](http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf), essentially created the field of information theory and is one of the best scientific papers ever written (in terms of both impact and readability).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovModel:\n",
    "    def __init__(self, n):\n",
    "        \"\"\"\n",
    "        Initialize the Markov model object.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n : int\n",
    "            the size of the ngram\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "\n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        Fit a Markov chain and create a transition matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            text to fit the Markov chain\n",
    "        \"\"\"\n",
    "\n",
    "        # make text circular so markov chain doesn't get stuck\n",
    "        circ_text = text + text[: self.n]\n",
    "#         circ_text = text\n",
    "        # count the number of occurrences of each letter following a given n-gram\n",
    "        frequencies = defaultdict(Counter)\n",
    "        for i in range(len(text)):\n",
    "            frequencies[circ_text[i : i + self.n]][circ_text[i + self.n]] += 1.0\n",
    "\n",
    "        # normalize the frequencies into probabilities (separately for each n-gram)\n",
    "        self.probabilities_ = defaultdict(dict)\n",
    "        for ngram, counts in frequencies.items():\n",
    "            self.probabilities_[ngram][\"symbols\"] = list(counts.keys())\n",
    "            probs = np.array(list(counts.values()))\n",
    "            probs /= np.sum(probs)\n",
    "            self.probabilities_[ngram][\"probs\"] = probs\n",
    "\n",
    "        # store the first n characters of the training text, as we will use these\n",
    "        # to seed our `generate` function\n",
    "        self.starting_chars = text[: self.n]\n",
    "        print(len(self.probabilities_))\n",
    "        self.frequencies_ = frequencies  # you never know when this might come in handy\n",
    "\n",
    "    def generate(self, seq_len):\n",
    "        \"\"\"\n",
    "        Using self.starting_chars, generate a sequence of length seq_len\n",
    "        using the transition matrix created in the fit method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seq_len : int\n",
    "            the desired length of the sequence\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        str\n",
    "            the generated sequence\n",
    "        \"\"\"\n",
    "        s = self.starting_chars\n",
    "        while len(s) < seq_len:\n",
    "            probs = self.probabilities_[s[-self.n :]]\n",
    "            s += npr.choice(probs[\"symbols\"], p=probs[\"probs\"])\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1557\n",
      "E Prope, ‘You up\n",
      "\n",
      "priall justleese bosece se evere cand ponce, at carrathe\n",
      "as lig, so stablover eve\n",
      "As anyth linge, and thew asithenintoods th moung crind heregifulew balred.\n",
      "\n",
      "withat countop carfs; a plead\n",
      "1.F.6. AND Tren and ther, she fet ong wher teriger, and don wast of it, the horrow,\n",
      "you was and wearnin the abin evere afte, had him dwas pas you shed: liked be\n",
      "is ang fir took doothe upon an to but mang.’ st might his st th of the my\n",
      "what ree mand he stlerldis ands pluck tone kin th\n"
     ]
    }
   ],
   "source": [
    "# Grimms' Fairy Tales by Jacob Grimm and Wilhelm Grimm\n",
    "data_url = \"http://www.gutenberg.org/files/2591/2591-0.txt\"\n",
    "\n",
    "corpus = urlopen(data_url).read().decode(\"utf-8\")\n",
    "corpus = corpus[2000:]\n",
    "model = MarkovModel(n=2)\n",
    "model.fit(corpus)\n",
    "print(model.generate(500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 \"Circular\" version of the text\n",
    "rubric={reasoning:5}\n",
    "\n",
    "**Your taks:**\n",
    "\n",
    "Why do we need to use a \"circular\" version of the text in the `fit` function? What could go wrong if we didn't do that, and instead used the original `text` (instead of `circ_text`) and made the loop:  \n",
    "```\n",
    "for i in range(len(text)-self.n):\n",
    "```\n",
    "which allow `fit` to run without crashing?\n",
    "\n",
    "> Hint: the problem would arise during `generate`, not during `fit`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution_2.1**\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "If we do not append the first n-gram at the end, then when we encounter the last n-gram in the generate function, there won't be any prediction letters for the last n-gram and the generate function will throw an error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) 2.2 Smoothing\n",
    "rubric={reasoning:1}\n",
    "\n",
    "The description above mentioned a connection to naive Bayes. When discussing naive Bayes we briefly mentioned [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing). Is there something analogous to Laplace smoothing that we could do here? If so, describe what it would entail and what effect it would have. (You don't have to implement it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution_2.2**\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "We could include all possible ngrams of length n, and add 1 to the count of all ngrams before normalizing. This will allow us to generate unseen patterns with very small but not zero probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 States, state space, and transition matrix\n",
    "rubric={reasoning:15}\n",
    "\n",
    "Let's consider the Markov chain interpretation of what we've just done. Let's define a state as the previous $n$ characters \"emitted\" by our chain. Let's assume our vocabulary size (the number of possible characters) is $V$; for example, $V=26$ if we were only using lowercase letters. We can compute this in our case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size = 86\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size = %d\" % len(np.unique(list(corpus))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's just stick with a general $V$ for now. Call the number of possible states $S$. Let's consider the _transition matrix_ $T$ of this Markov chain. The transition matrix is a square matrix and the number of rows/columns is equal to the number of states, so in our case it is $S \\times S$. Each row is a discrete probability distribution summing to 1. The element $T_{ij}$ is the probability of transitioning from state $i$ to state $j$. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. What is the number of possible states, $S$, in terms of $V$ and $n$? (You might not encounter all these states in practice, if not all $n$-grams appear in the training data, but let's ignore that for now.)\n",
    "2. Give two examples states in the state space for $n=3$.\n",
    "3. In our application above when $n>1$ _not all transitions are possible_. This means $T$ will contain a bunch of zeros. Why?\n",
    "4. Again for $n>1$ what is the maximum number of nonzero elements that $T$ could have? Answer in terms of $V$ and $n$. \n",
    "5. Is the state space discrete or continuous?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE\n",
    "\n",
    "1. V^n\n",
    "\n",
    "2. are, abl\n",
    "\n",
    "3. When n = 3 for instance, and the current state is \"are\", the next state cannot be something like \"bcd\" because it should have atleast two characters in common with the previous state, since only the last character is being generated. The next state can be \"re\" followed by another character that is generated.\n",
    "\n",
    "4. (V^n) * V\n",
    "\n",
    "5. The state space is continuous because there are common characters between one state and the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Stationary distribution and other fun stuff <a name=\"3\"></a>\n",
    "\n",
    "The code below computes the transition matrix for you for the Shakespeare data with `n=1`. Consider this transition matrix for the next few questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Project Gutenberg eBook of The Complete Works of William Shakespeare, by William Shakespeare\n",
    "data_url = \"http://www.gutenberg.org/files/100/100-0.txt\"\n",
    "shakespeare_text = urlopen(data_url).read().decode(\"utf-8\")\n",
    "shakespeare_text = shakespeare_text[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: ['\\t' '\\n' '\\r' ' ' '!' '\"' '$' '%' '&' \"'\" '(' ')' '*' ',' '-' '.' '/'\n",
      " '0' '1' '2' '3' '4' '5' '6' '7' '8' '9' ':' ';' '?' '@' 'A' 'B' 'C' 'D'\n",
      " 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V'\n",
      " 'W' 'X' 'Y' 'Z' '[' '\\\\' ']' '_' '`' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i'\n",
      " 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z' '|'\n",
      " '}' 'Æ' 'É' 'à' 'â' 'æ' 'ç' 'è' 'é' 'ê' 'ë' 'î' 'œ' '—' '‘' '’' '“' '”']\n",
      "Number of states: 107\n",
      "107\n",
      "Number of nonzero transition probabilities: 2647/11449\n"
     ]
    }
   ],
   "source": [
    "states = np.unique(list(shakespeare_text))\n",
    "print(\"States:\", states)\n",
    "S = len(np.unique(list(shakespeare_text)))\n",
    "print(\"Number of states:\", S)\n",
    "\n",
    "model = MarkovModel(n=1)\n",
    "model.fit(shakespeare_text)\n",
    "\n",
    "# implementation note: since len(model.probabilities_[state][\"probs\"]) might not equal S\n",
    "# for all letters, we need to be careful and do a reverse-lookup for the actual letters\n",
    "# the rest of the transition probabilities are just zero (if they don't appear)\n",
    "lookup = dict(zip(states, list(np.arange(S, dtype=int))))\n",
    "\n",
    "T = np.zeros((S, S))  # transition matrix\n",
    "for i, state in enumerate(states):\n",
    "    for j, letter in enumerate(model.probabilities_[state][\"symbols\"]):\n",
    "        T[i, lookup[letter]] = model.probabilities_[state][\"probs\"][j]\n",
    "\n",
    "print(\"Number of nonzero transition probabilities: %d/%d\" % (np.sum(T > 0), T.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Stationary distribution conditions \n",
    "rubric={reasoning:5}\n",
    "\n",
    "Under mild assumptions, a Markov chain has a _stationary distribution_ which is the probability of finding yourself in the same state after the chain is run for a long time. These assumptions are:\n",
    "\n",
    "- \"Irreducibility\" (doesn’t get stuck in part of the graph)\n",
    "- \"Aperiodicity\" (doesn’t keep repeating same sequence).\n",
    "\n",
    "Give a short explanation for why our Markov chain might satisfy these assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE\n",
    "\n",
    "Irreducibility: Every state(n-gram) that occurs is part of the frequencies dictionary. There is no state that is not part of the dictionary and hence every state has some predictions for the next character with certain probability. There is very low probability that two states will have only each other as their predictions. Hence, there is no probability of the graph getting stuck in some sequence.\n",
    "\n",
    "Aperiodicity: Every state has the next character that might occur as a discrete probability set. It can be any character from that set. So, the process seems to be randomly picking a character from the values associated with the current state and performing thhis repeatedly. There is very low probability that a certain pattern might occur at **regular** intervals given the process is random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Stationary distribution for the `shakespeare_text`\n",
    "rubric={accuracy:10}\n",
    "\n",
    "It's not true in general but in this particular case we actually already know the stationary distribution -- it is just the relative frequency that these n-grams occur in the training data (you are welcome to think about this deeply, but you don't have to). (Recall that we are assuming n=1 here.)\n",
    "\n",
    "**Your tasks:** \n",
    "\n",
    "1. Calculate the stationary distribution for the `shakespeare_text` by calculating the relative frequencies for each state in the corpus (unique letter in this case, as we are assuming `n=1`).\n",
    "2. Show empirically that this is a stationary distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.25125321e-07 2.98348201e-02 2.98348201e-02 1.84509258e-01\n",
      " 1.49013062e-03 1.40033419e-05 3.50083547e-07 1.75041774e-07\n",
      " 1.15527571e-05 2.49189469e-03 5.74137018e-05 5.72386600e-05\n",
      " 4.55108612e-06 1.62132443e-02 1.01104129e-03 1.48939544e-02\n",
      " 2.10050128e-06 2.87068509e-05 6.16147043e-05 3.97344826e-05\n",
      " 2.87068509e-05 3.37830623e-05 2.03048458e-05 2.71314749e-05\n",
      " 1.54036761e-05 2.59061825e-05 1.61038432e-05 8.24796838e-04\n",
      " 2.99093879e-03 1.94313873e-03 1.75041774e-07 7.78095693e-03\n",
      " 2.32087888e-03 3.28413376e-03 2.30232445e-03 6.31375678e-03\n",
      " 1.98112279e-03 1.82165974e-03 2.94070180e-03 9.06016221e-03\n",
      " 3.25052574e-04 1.00929087e-03 3.81328504e-03 2.53985614e-03\n",
      " 4.35959042e-03 4.84515630e-03 1.84424013e-03 2.06199209e-04\n",
      " 4.22288279e-03 5.34472552e-03 6.74610996e-03 2.24858663e-03\n",
      " 5.58033175e-04 2.91129478e-03 6.42403310e-05 1.27045319e-03\n",
      " 9.87235604e-05 6.09670498e-04 1.75041774e-07 6.09845540e-04\n",
      " 9.38048865e-04 1.75041774e-07 4.64706152e-02 8.88109447e-03\n",
      " 1.28183091e-02 2.54941342e-02 7.80336227e-02 1.30897989e-02\n",
      " 1.09199811e-02 4.18096029e-02 3.80941662e-02 5.33352285e-04\n",
      " 5.60151180e-03 2.78272660e-02 1.80476821e-02 4.12594466e-02\n",
      " 5.33947427e-02 8.93465726e-03 4.85740922e-04 3.98897447e-02\n",
      " 4.13504683e-02 5.52307558e-02 2.17984772e-02 6.48582284e-03\n",
      " 1.39287741e-02 8.74333660e-04 1.61437527e-02 2.17576925e-04\n",
      " 1.75041774e-07 3.50083547e-07 5.25125321e-07 8.75208869e-07\n",
      " 2.27554306e-06 3.50083547e-07 8.05192159e-06 1.57537596e-06\n",
      " 6.65158740e-06 7.00167095e-06 8.75208869e-07 1.75041774e-07\n",
      " 1.75041774e-07 8.75208869e-07 2.88818927e-04 6.33651221e-05\n",
      " 2.99583996e-03 8.19195501e-05 6.98416677e-05]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "### YOUR ANSWER HERE\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "total_chars = len(shakespeare_text)\n",
    "stationary_dist_freq = np.array(\n",
    "    [shakespeare_text.count(state) / total_chars for state in states]\n",
    ")\n",
    "print(stationary_dist_freq)\n",
    "\n",
    "# pi = pd.Series(lookup.values())/total_chars\n",
    "# pi.index = lookup.keys()\n",
    "# pi = np.array(pi)\n",
    "\n",
    "# print(np.allclose(pi @ T, pi))\n",
    "\n",
    "print(np.allclose(stationary_dist_freq @ T, stationary_dist_freq))\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equality statement evaluated to True. Hence, empirically proved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 3.3 Stationary distribution using eigenvalue decomposition\n",
    "rubric={reasoning:1}\n",
    "\n",
    "You can compute the stationary distribution in another ways. \n",
    "\n",
    "**Your tasks**\n",
    "\n",
    "1. Calculate stationary distribution using eigenvalue decomposition of the transition matrix. Compare your result from part 3.2 with the result below. Do they agree with each other?\n",
    "\n",
    "> We haven't explicitly talked about this in class but at this point in the program, I believe that you are independent enough to do it on your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution_3_3_1\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Finding probability of occurrences of patterns \n",
    "rubric={reasoning:15}\n",
    "\n",
    "Let's consider the conditional probability that a lower case vowel comes 3 characters later given that the current letter is \"a\". In other words, we're searching for the pattern `axxv` where `v` is a lower case vowel (defined as a,e,i,o,u,y) and `x` is any character and `a` is literally \"a\". \n",
    "\n",
    "Let's use `n=1`.\n",
    "\n",
    "**Your tasks:**\n",
    "1. It turns out we can estimate this probability directly from the transition matrix. While $T$ gives us the probabilities one step ahead, $T\\times T$ gives us the probabilities 2 steps ahead, etc. (If you want to think about this, you should be able to convince yourself it's true by contemplating what matrix multiplication really means, but this is optional.) So, taking $T^3$ gives us the transition probabilities for 3 steps later. Compute $T^3$ and find the estimated probability that we're looking for. \n",
    "2. We could also estimate this probability directly from the data, by just seeing how frequently this pattern occurs. Do this. How well does it match your above results? What does this tell us about how reasonable our  Markov assumption is?\n",
    "3. What if we increased `n` and repeated part (1), would you expect to get closer to the answer from part (2)? You are welcome to try this but you don't have to - the goal is just to think about it.\n",
    "\n",
    "> Note for 3.4.1: you should NOT use `T**3`, as that is element-wise exponentiation rather than a matrix power. You can get $T^3$ using `numpy.linalg.matrix_power(T,3)` or just `T@T@T`.\n",
    "\n",
    "> This question might be a bit tricky. But hopefully it will help you understand Markov models better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25512535358697536"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Solution_3_4_1\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "probs = np.linalg.matrix_power(T,3)\n",
    "probs_df = pd.DataFrame(data = probs, index = states, columns = states)\n",
    "probs_df.loc['a'].filter(items = ['a', 'e', 'i', 'o', 'u', 'y']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2548411762711737\n"
     ]
    }
   ],
   "source": [
    "### Solution_3_4_2\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "count = 0\n",
    "context = 0\n",
    "for i, char in enumerate(shakespeare_text):\n",
    "    if char == 'a' and ((i + 3) < len(shakespeare_text)):\n",
    "        count += 1\n",
    "        if (shakespeare_text[i+3] in ['a', 'e', 'i', 'o', 'u', 'y']):\n",
    "            context += 1\n",
    "print(context/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtained answer is very close to the one obtained from the transition matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution_3_4_3**\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "When we increase n, there are a lot more characters and the context is more prevalent than in the case with n = 1. Hence, it is easier to predict the next characters. Hence, I think the answers will be more closer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Markov model of language with words <a name=\"4\"></a>\n",
    "\n",
    "In this exercise we'll continue with the Markov model of language, but we'll work with _words_ instead of _characters_. The `MarkovModel` code stays the same. Just remember that now we are dealing with words and not characters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we say n-gram we're now referring to `n` words, not `n` characters.\n",
    "\n",
    "One concern with words is that, in a sense, we have less examples to work with. For example, with character n-grams and `n=3` we have a lot of examples of seeing the character combination \"per\" and checking what character follows it, but with words even with `n=1` (the smallest nontrivial value of `n`), we might only have one example of the word \"persuade\" in our corpus. You can imagine that the number of \"training examples\" only gets smaller if `n`>1. This is something to keep in mind when deciding what values of `n` might be reasonable.\n",
    "\n",
    "Another issue is that the number of states could explode when you are working with words. For example, with character bigram model our states were all unique characters in the text (107 in our example above). For word bigram model, it's going to be the number of unique words in the corpus, which would be a much bigger number. You can imagine that for a large corpus and bigger values of `n`, the number of states could explode.  \n",
    "\n",
    "Finally, we need to preprocess the text (i.e., tokenization) before creating a word-based n-gram model. To accomplish this preprocessing, we will use the popular python package [NLTK](http://www.nltk.org/) (Natural Language ToolKit), which we have seen in class. If you are using the course conda environment, you should already have the package in your conda environment. \n",
    "\n",
    "You might need to install the NLTK data files with the following command in your conda environment in the terminal.\n",
    "\n",
    "```\n",
    "python -m nltk.downloader 'punkt'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Word-based n-gram language model\n",
    "rubric={accuracy:8,reasoning:4}\n",
    "\n",
    "The first step is to break apart the text string into words. NLTK's `word_tokenize` will turn our text into a lists of \"tokens\" which we will use as our language units. This tokenizing process removes whitespace. So, in `generate`, you should add a space character back in between tokens. This won't look quite right because you'll have spaces before punctuations, but it's good enough for now.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Implement a word-based Markov model of language.\n",
    "2. Train your word-based Markov model on `shakespeare_text` above with your choice of `n` and generate some text. \n",
    "3. Discuss the differences between character-based and word-based Markov models in terms of generated text, the number of states, and the time taken to generate the text. \n",
    "\n",
    "> Note: You can reuse the code from Exercise 2. Only the `generate` function needs to be rewritten, and, even there, the required modifications are not large. You may use class inheritance you have learned in 511 here. The line\n",
    "```\n",
    "class MarkovModelWords(MarkovModel):\n",
    "```\n",
    "> defines a new class called `MarkovModelWords` that you can think of as an offspring of `MarkovModel`. It \"inherits\" all the functions from `MarkovModel` unless we explicitly overwrite them. So, if you implement `generate` in the class `MarkovModelWords`, the old `generate` function will be overwritten but all the other functions will persist. But, if you're not comfortable with inheritance, or just don't feel like it, you're  welcome to just modify the above code and not use inheritance.\n",
    "\n",
    "\n",
    "> Another note: The way `MarkovModel` is implemented, the patterns we're conditioning on (the last $n$ characters/words) are used as keys in python dictionaries. For characters that was easy because we could just use a string as a key. However, for words with $n>1$ we now have a collection of words, and we want to use this as a dict key. `nltk.tokenize.word_tokenize` outputs a list, which can't be used as a key. I've casted the list to a tuple in the code below, because a tuple _can_ be used as a dict key (if you case: I believe this is because it's immutable whereas a list is not). This tiny change allows reuse of the old code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text_tok = word_tokenize(shakespeare_text)\n",
    "text_tok = tuple(text_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution_4_1_1\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "class MarkovModelWords:\n",
    "    def __init__(self, n):\n",
    "        \"\"\"\n",
    "        Initialize the Markov model object.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n : int\n",
    "            the size of the ngram\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "\n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        Fit a Markov chain and create a transition matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            text to fit the Markov chain\n",
    "        \"\"\"\n",
    "\n",
    "        # make text circular so markov chain doesn't get stuck\n",
    "        circ_text = text + text[: self.n]\n",
    "#         circ_text = text\n",
    "        # count the number of occurrences of each letter following a given n-gram\n",
    "        frequencies = defaultdict(Counter)\n",
    "        for i in range(len(text)):\n",
    "            frequencies[circ_text[i : i + self.n]][circ_text[i + self.n]] += 1.0\n",
    "\n",
    "        # normalize the frequencies into probabilities (separately for each n-gram)\n",
    "        self.probabilities_ = defaultdict(dict)\n",
    "        for ngram, counts in frequencies.items():\n",
    "            self.probabilities_[ngram][\"symbols\"] = list(counts.keys())\n",
    "            probs = np.array(list(counts.values()))\n",
    "            probs /= np.sum(probs)\n",
    "            self.probabilities_[ngram][\"probs\"] = probs\n",
    "        # store the first n characters of the training text, as we will use these\n",
    "        # to seed our `generate` function\n",
    "        self.starting_chars = text[: self.n]\n",
    "        self.frequencies_ = frequencies  # you never know when this might come in handy\n",
    "\n",
    "    def generate(self, seq_len):\n",
    "        \"\"\"\n",
    "        Using self.starting_chars, generate a sequence of length seq_len\n",
    "        using the transition matrix created in the fit method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seq_len : int\n",
    "            the desired length of the sequence\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        str\n",
    "            the generated sequence\n",
    "        \"\"\"\n",
    "#         s = self.starting_chars\n",
    "#         count = self.n\n",
    "#         while count < seq_len:\n",
    "#             result = s[-self.n:]\n",
    "#             word_gram = \" \".join(result)\n",
    "#             probs = self.probabilities_[word_gram]\n",
    "#             print(probs)\n",
    "#             s += list(npr.choice(probs[\"symbols\"], p=probs[\"probs\"]))\n",
    "#             count += 1\n",
    "#         return s\n",
    "        count = self.n\n",
    "        s = list(self.starting_chars)\n",
    "        while count < seq_len:\n",
    "            probs = self.probabilities_[tuple(s[-self.n :])]\n",
    "#             print(f\"\"\"n-gram : {tuple(s[-self.n :])} corresponding value : {probs}\"\"\")\n",
    "            pred = npr.choice(probs[\"symbols\"], p=probs[\"probs\"])\n",
    "            s += [pred]\n",
    "            count += 1\n",
    "        s = \" \".join(s)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'serv ’ d him at the barber ’ s shop is hanging still , That mouldeth goblins swift as frenzy ’ s thoughts . Strike a free march to Troy . With comfort go ; Hope of revenge shall hide our inward woe . Enter Pandarus . CRESSIDA . A'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Solution_4_1_2\n",
    "### YOUR ANSWER HERE\n",
    "model = MarkovModelWords(n=3)\n",
    "model.fit(text_tok)\n",
    "result = model.generate(50)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution_4_1_3**\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "When I printed the length of the frequencies dictionary for word based model, the length is much higher than that of character based. I think this is because a lot more combinations are possible in word based model as compared to character based model. The time taken to fit is lesser than the time taken for charcter based model. The generate function speed is not very different for both the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Dealing with punctuation\n",
    "rubric={accuracy:10,reasoning:4}\n",
    "\n",
    "If you've generated text from 4.1, you probably noticed that the way whitespace is handled is not ideal. In particular, whitespace is added after every token, including right before punctuation. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Modify your code to fix this and show a generated sequence. You can access a list of punctuation characters with `string.punctuation` from the [`string` library](https://docs.python.org/3/library/string.html).\n",
    "2. Generate text using different values of `n` on `shakespeare_text`.  \n",
    "3. Train word-based Markov models on another corpus of your choice and generate text with different values of `n`. You may use any corpus of your choice. For inspiration, you may find some corpora [here](http://www.gutenberg.org/). \n",
    "4. Note your observations on \n",
    "    - how the generated text changes based on the corpus \n",
    "    - how the value of `n` affects the quality of the generated text and time taken to generate the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution_4_2_1\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "### YOUR ANSWER HERE\n",
    "class MarkovModelWordsPunc:\n",
    "    def __init__(self, n):\n",
    "        \"\"\"\n",
    "        Initialize the Markov model object.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        n : int\n",
    "            the size of the ngram\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "\n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        Fit a Markov chain and create a transition matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text : str\n",
    "            text to fit the Markov chain\n",
    "        \"\"\"\n",
    "\n",
    "        # make text circular so markov chain doesn't get stuck\n",
    "        circ_text = text + text[: self.n]\n",
    "#         circ_text = text\n",
    "        # count the number of occurrences of each letter following a given n-gram\n",
    "        frequencies = defaultdict(Counter)\n",
    "        for i in range(len(text)):\n",
    "            frequencies[circ_text[i : i + self.n]][circ_text[i + self.n]] += 1.0\n",
    "\n",
    "        # normalize the frequencies into probabilities (separately for each n-gram)\n",
    "        self.probabilities_ = defaultdict(dict)\n",
    "        for ngram, counts in frequencies.items():\n",
    "            self.probabilities_[ngram][\"symbols\"] = list(counts.keys())\n",
    "            probs = np.array(list(counts.values()))\n",
    "            probs /= np.sum(probs)\n",
    "            self.probabilities_[ngram][\"probs\"] = probs\n",
    "        # store the first n characters of the training text, as we will use these\n",
    "        # to seed our `generate` function\n",
    "        self.starting_chars = text[: self.n]\n",
    "        self.frequencies_ = frequencies  # you never know when this might come in handy\n",
    "\n",
    "    def generate(self, seq_len):\n",
    "        \"\"\"\n",
    "        Using self.starting_chars, generate a sequence of length seq_len\n",
    "        using the transition matrix created in the fit method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seq_len : int\n",
    "            the desired length of the sequence\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        str\n",
    "            the generated sequence\n",
    "        \"\"\"\n",
    "        count = self.n\n",
    "        s = list(self.starting_chars)\n",
    "        while count < seq_len:\n",
    "            probs = self.probabilities_[tuple(s[-self.n :])]\n",
    "            pred = npr.choice(probs[\"symbols\"], p=probs[\"probs\"])\n",
    "            s += [pred]\n",
    "            count += 1\n",
    "        s = \" \".join(s)\n",
    "        expr = f'''\\s+(?=[{string.punctuation}'])'''\n",
    "        s = re.sub(expr, '', s)\n",
    "        expr2 = f'''\\'\\s+'''\n",
    "        s = re.sub(expr2, '\\'', s)\n",
    "        return s\n",
    "# class MarkovModelWords(MarkovModel):\n",
    "#     def generate(self, seq_len):\n",
    "#         n = self.n\n",
    "#         seq = self.starting_chars\n",
    "#         while len(seq) < seq_len:\n",
    "#             probs = self.probabilities_[seq[-n:]]\n",
    "#             seq += (npr.choice(probs[\"symbols\"], p=probs[\"probs\"]),)\n",
    "#         output = seq[0]\n",
    "#         for s in seq[1:]:\n",
    "#             if s[0] in string.punctuation:\n",
    "#                 output += s\n",
    "#             else:\n",
    "#                 output += \" \" + s\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All were surprised by Merriwell ’ s final words were a thrust at him. The owner of those eyes drew back in a moment. Ha! I thought as much! I saw you down at the wharf, Dustan met him at the head of the stairs'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Solution_4_2_2\n",
    "### YOUR ANSWER HERE\n",
    "model = MarkovModelWordsPunc(n=3)\n",
    "model.fit(text_tok)\n",
    "result = model.generate(50)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All were surprised by Merriwell ’ s sudden move. Frank had seen a person appear in the open door of the freight house, look at him, and then dodge back. Although he obtained but a glimpse of this person, Merry fancied he knew him. Into the doorway he sprang, and looked around. On every hand were boxes and barrels and piles of freight, but no one was to be seen. The opposite door was standing open. “ Must have dodged out that way, ” muttered Frank, and he darted toward the door. But when he reached the door, he looked in vain for the person he fancied he had seen. “ My eyes may have fooled me, ” he said. “ We have a hard hill out here, ” said Woodock. “ Some of us can ’ t climb it, ” smiled Frank. “ I think there is no danger of your failing. ” They started. From choice it seemed Mabel Mischief rode at Frank ’ s side, chatting with Hattie Hazle, who was next to her. Hattie had dark eyes and hair, presenting a strong contrast to the lively blonde. The hill proved to be rather steep and difficult, but Frank and Dustan pedalled up it without permitting their wheels to falter or sway on the steepest grade. The girls found it more difficult, but they stuck to it persistently till they accomplished the ascent. Looking back, it was found that more than half the party was trundling the wheels, so those who had ridden up dismounted and waited for the others. With the hill behind them, the party was formed again, and then they rode along the elm-shaded streets into the business portion of the city. “ Will you ride about the city, or shall we go directly to the club rooms? ” asked Dustan. “ I would enjoy seeing something of the city. ” “ That will suit us. ” So they rode about the place, and Frank was pleased with its general appearance. At last they arrived at the club rooms of the Belfast Wheelmen, and there the girls left them, Mabel and Hattie bidding Frank good-by, but expressing a hope that they might see him again. Dustan led the way into the club rooms, and Merry followed. Just as Frank was entering, a person attempted to come out hurriedly and brush past him. With an exclamation of satisfaction, Frank grasped this individual, holding him fast. “ Wait a minute, ” cried Merry. “ I think I know you! ” “ Leggo! ” snarled the man. “ In a moment. Ha! I thought as much\n"
     ]
    }
   ],
   "source": [
    "### Solution_4_2_3\n",
    "### YOUR ANSWER HERE\n",
    "import urllib.request\n",
    "sample_data_url = 'http://www.gutenberg.org/files/63483/63483-0.txt'\n",
    "sample_corpus = urllib.request.urlopen(sample_data_url).read().decode(\"utf-8\")\n",
    "sample_corpus = sample_corpus[9300:34900]\n",
    "text_tok = word_tokenize(sample_corpus)\n",
    "text_tok = tuple(text_tok)\n",
    "sample_model = MarkovModelWordsPunc(n = 5)\n",
    "sample_model.fit(text_tok)\n",
    "sample_result = sample_model.generate(500)\n",
    "print(sample_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution_4_2_4**\n",
    "### YOUR ANSWER HERE\n",
    "\n",
    "As n increases, the quality of text generated, significantly increases. Also, with classic english literature, the generated text seems to be of not very high quality. With other corpus, the text generated seems to be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 4.3 Other improvements\n",
    "rubric={reasoning:1}\n",
    "\n",
    "Are there other improvements you'd like to make to your word-based Markov model so that the generated text looks like proper English? Feel free to either implement them or just describe what you might want to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution_4_3**\n",
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission to Canvas <a name=\"sc\"></a>\n",
    "\n",
    "**PLEASE READ: When you are ready to submit your assignment do the following:**\n",
    "\n",
    "- Run all cells in your notebook to make sure there are no errors by doing Kernel -->  Restart Kernel and Run All Cells...\n",
    "- If you are using the \"575\" `conda` environment, make sure to select it before running all cells. \n",
    "- Convert your notebook to .html format using the `convert_notebook()` function below or by File -> Export Notebook As... -> Export Notebook to HTML\n",
    "- Run the code `submit()` below to go through an interactive submission process to Canvas.\n",
    "After submission, be sure to do a final push of all your work to GitHub (including the rendered html file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from canvasutils.submit import convert_notebook, submit\n",
    "\n",
    "# convert_notebook(\"lab1.ipynb\", \"html\")  # uncomment and run when you want to try convert your notebook (or you can convert manually from the File menu)\n",
    "# submit(course_code=65512, token=False)  # uncomment and run when ready to submit to Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done!! Congratulations on finishing the lab!! "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:575]",
   "language": "python",
   "name": "conda-env-575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
